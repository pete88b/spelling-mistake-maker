{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26d7464d",
   "metadata": {},
   "source": [
    "# Reproduce `ZeroDivisionError` in `NotebookProgressBar`\n",
    "\n",
    "I think the key ingredients to reproducing this error are\n",
    "- small model\n",
    "- small batches of data / small sequence length\n",
    "- frequent evaluation and logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5987dc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers, json, time\n",
    "from datasets import Dataset\n",
    "from typing import Dict, List, Optional, Sequence, Union"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c26152",
   "metadata": {},
   "source": [
    "## `CharacterTokenizer`\n",
    "\n",
    "Create a character level tokenizer based on [character tokenizer](https://raw.githubusercontent.com/dariush-bahrami/character-tokenizer/master/charactertokenizer/core.py) for BERT (which was inspired by the [CANINE](https://arxiv.org/abs/2103.06874) [tokenizer](https://github.com/huggingface/transformers/blob/main/src/transformers/models/canine/tokenization_canine.py)) and the [t5 tokenizer](https://github.com/huggingface/transformers/blob/main/src/transformers/models/t5/tokenization_t5.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42399f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterTokenizer(transformers.tokenization_utils.PreTrainedTokenizer):\n",
    "    def __init__(self, characters: Sequence[str], model_max_length: int, **kwargs):\n",
    "        \"\"\"Character tokenizer for Hugging Face transformers.\n",
    "\n",
    "        Args:\n",
    "            characters (Sequence[str]): List of desired characters. Any character which\n",
    "                is not included in this list will be replaced by a special token <unk>.\n",
    "\n",
    "            model_max_length (int): Model maximum sequence length.\n",
    "        \"\"\"\n",
    "        self.characters = characters\n",
    "        self.model_max_length = model_max_length\n",
    "        pad_token = \"<pad>\"\n",
    "        unk_token = \"<unk>\"\n",
    "        eos_token = \"</s>\"\n",
    "        for token in [pad_token, unk_token, eos_token]:\n",
    "            transformers.tokenization_utils.AddedToken(token)\n",
    "        \n",
    "        self._vocab_str_to_int = {\n",
    "            pad_token: 0,\n",
    "            eos_token: 1,\n",
    "            unk_token: 2,\n",
    "            **{ch: i + 3 for i, ch in enumerate(characters)},\n",
    "        }\n",
    "        self._vocab_int_to_str = {v: k for k, v in self._vocab_str_to_int.items()}\n",
    "        \n",
    "        super().__init__(\n",
    "            eos_token=eos_token,\n",
    "            pad_token=pad_token,\n",
    "            unk_token=unk_token,\n",
    "            add_prefix_space=False,\n",
    "            model_max_length=model_max_length,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self) -> int:\n",
    "        return len(self._vocab_str_to_int)\n",
    "\n",
    "    def get_vocab(self):\n",
    "        return self._vocab_str_to_int\n",
    "    \n",
    "    def _tokenize(self, text: str) -> List[str]:\n",
    "        return list(text)\n",
    "\n",
    "    def _convert_token_to_id(self, token: str) -> int:\n",
    "        return self._vocab_str_to_int.get(token, 2) # default to unk_token <unk>\n",
    "\n",
    "    def _convert_id_to_token(self, index: int) -> str:\n",
    "        return self._vocab_int_to_str[index]\n",
    "\n",
    "    def convert_tokens_to_string(self, tokens):\n",
    "        return \"\".join(tokens)\n",
    "\n",
    "    def _add_eos_if_not_present(self, token_ids: List[int]) -> List[int]:\n",
    "        \"\"\"Do not add eos again if user already added it.\"\"\"\n",
    "        if len(token_ids) > 0 and token_ids[-1] == self.eos_token_id:\n",
    "            return token_ids\n",
    "        else:\n",
    "            return token_ids + [self.eos_token_id]\n",
    "        \n",
    "    def build_inputs_with_special_tokens(\n",
    "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
    "    ) -> List[int]:\n",
    "        \"\"\"\n",
    "        Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n",
    "        adding special tokens. A sequence has the following format:\n",
    "\n",
    "        - single sequence: `X </s>`\n",
    "        - pair of sequences: `A </s> B </s>`\n",
    "\n",
    "        Args:\n",
    "            token_ids_0 (`List[int]`):\n",
    "                List of IDs to which the special tokens will be added.\n",
    "            token_ids_1 (`List[int]`, *optional*):\n",
    "                Optional second list of IDs for sequence pairs.\n",
    "\n",
    "        Returns:\n",
    "            `List[int]`: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.\n",
    "        \"\"\"\n",
    "        token_ids_0 = self._add_eos_if_not_present(token_ids_0)\n",
    "        if token_ids_1 is None:\n",
    "            return token_ids_0\n",
    "        else:\n",
    "            token_ids_1 = self._add_eos_if_not_present(token_ids_1)\n",
    "            return token_ids_0 + token_ids_1\n",
    "        \n",
    "    def get_special_tokens_mask(\n",
    "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None, already_has_special_tokens: bool = False\n",
    "    ) -> List[int]:\n",
    "        \"\"\"\n",
    "        Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding\n",
    "        special tokens using the tokenizer `prepare_for_model` method.\n",
    "\n",
    "        Args:\n",
    "            token_ids_0 (`List[int]`):\n",
    "                List of IDs.\n",
    "            token_ids_1 (`List[int]`, *optional*):\n",
    "                Optional second list of IDs for sequence pairs.\n",
    "            already_has_special_tokens (`bool`, *optional*, defaults to `False`):\n",
    "                Whether or not the token list is already formatted with special tokens for the model.\n",
    "\n",
    "        Returns:\n",
    "            `List[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n",
    "        \"\"\"\n",
    "        if already_has_special_tokens:\n",
    "            return super().get_special_tokens_mask(\n",
    "                token_ids_0=token_ids_0, token_ids_1=token_ids_1, already_has_special_tokens=True\n",
    "            )\n",
    "\n",
    "        # normal case: some special tokens\n",
    "        if token_ids_1 is None:\n",
    "            return ([0] * len(token_ids_0)) + [1]\n",
    "        return ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n",
    "\n",
    "    def create_token_type_ids_from_sequences(\n",
    "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
    "    ) -> List[int]:\n",
    "        \"\"\"\n",
    "        Create a mask from the two sequences passed to be used in a sequence-pair classification task. T5 does not make\n",
    "        use of token type ids, therefore a list of zeros is returned.\n",
    "\n",
    "        Args:\n",
    "            token_ids_0 (`List[int]`):\n",
    "                List of IDs.\n",
    "            token_ids_1 (`List[int]`, *optional*):\n",
    "                Optional second list of IDs for sequence pairs.\n",
    "\n",
    "        Returns:\n",
    "            `List[int]`: List of zeros.\n",
    "        \"\"\"\n",
    "        eos = [self.eos_token_id]\n",
    "\n",
    "        if token_ids_1 is None:\n",
    "            return len(token_ids_0 + eos) * [0]\n",
    "        return len(token_ids_0 + eos + token_ids_1 + eos) * [0]\n",
    "\n",
    "    def get_config(self) -> Dict:\n",
    "        return {\n",
    "            \"char_ords\": [ord(ch) for ch in self.characters],\n",
    "            \"model_max_length\": self.model_max_length,\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config: Dict) -> \"CharacterTokenizer\":\n",
    "        cfg = {}\n",
    "        cfg[\"characters\"] = [chr(i) for i in config[\"char_ords\"]]\n",
    "        cfg[\"model_max_length\"] = config[\"model_max_length\"]\n",
    "        return cls(**cfg)\n",
    "\n",
    "    def save_pretrained(self, save_directory: Union[str, os.PathLike], **kwargs):\n",
    "        cfg_file = Path(save_directory) / \"tokenizer_config.json\"\n",
    "        cfg = self.get_config()\n",
    "        with open(cfg_file, \"w\") as f:\n",
    "            json.dump(cfg, f, indent=4)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, save_directory: Union[str, os.PathLike], **kwargs):\n",
    "        cfg_file = Path(save_directory) / \"tokenizer_config.json\"\n",
    "        with open(cfg_file) as f:\n",
    "            cfg = json.load(f)\n",
    "        return cls.from_config(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b38ee2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '-']\n"
     ]
    }
   ],
   "source": [
    "characters = [chr(i) for i in range(65, 91)] + ['-']\n",
    "print(characters)\n",
    "tokenizer = CharacterTokenizer(characters, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b5476b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# go for a v-small model of ~1m parameters - T5-Small (60 million parameters)\n",
    "# TODO: not sure if these numbers give us the best model for the param budget ...\n",
    "model_config = transformers.models.t5.configuration_t5.T5Config(\n",
    "    decoder_start_token_id=tokenizer.pad_token_id,\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    d_model=128,                        # \"d_model\": 512,\n",
    "    d_kv=16,                            # \"d_kv\": 64,\n",
    "    d_ff=512,                           # \"d_ff\": 2048,\n",
    "    num_layers=3,                       # \"num_layers\": 6,\n",
    "    num_heads=4,                        # \"num_heads\": 8,\n",
    "    relative_attention_num_buckets=8,   # \"relative_attention_num_buckets\": 32,\n",
    "    relative_attention_max_distance=32, # relative_attention_max_distance (`int`, *optional*, defaults to 128):\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b8bf559",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dict(\n",
    "    source=['EXCEPT', 'EXERCISE', 'EXHILARATION', 'EXISTENCE', 'EXISTENCE', 'EXPEDIENT', 'EXPLICITLY', 'EXTENSIONS'] * 999,\n",
    "    target=['EXCPT', 'EXCERCISE', 'EXHILERATION', 'EGSISTENCE', 'EXISTANCE', 'EXSPIDIENT', 'EXPLEKIYLY', 'EXTIONS'] * 999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5f82d13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['source', 'target'],\n",
       "        num_rows: 7192\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['source', 'target'],\n",
       "        num_rows: 800\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset.from_dict(data).train_test_split(test_size=0.1, shuffle=True)\n",
    "train_dataset = dataset['train']\n",
    "val_dataset = dataset['test']\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41bea1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = None # 128??\n",
    "max_target_length = None # 128?\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    model_inputs = tokenizer(examples['source'], max_length=max_input_length, truncation=True)\n",
    "    labels = tokenizer(examples['target'], max_length=max_target_length, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef6cc0ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2683bf7867c4493bae19f3a8c6906ca6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7192 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f2afe8dd7a2433488092c91195033dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=['source', 'target']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e982564",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_model_name = f'models/spelling-corrector/quick1'\n",
    "args = transformers.Seq2SeqTrainingArguments(\n",
    "    finetune_model_name,\n",
    "    optim=\"adamw_torch\",\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=100,\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-3,\n",
    "    per_device_train_batch_size=13,\n",
    "    per_device_eval_batch_size=13,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=1,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    push_to_hub=False,\n",
    "    logging_steps=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f933c1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='201' max='554' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [198/554 00:05 < 00:10, 34.08 it/s, Epoch 0.36/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.109000</td>\n",
       "      <td>0.029047</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='62' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1/62 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 10\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mAutoModelForSeq2SeqLM\u001b[38;5;241m.\u001b[39mfrom_config(model_config)\n\u001b[0;32m      2\u001b[0m trainer \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mSeq2SeqTrainer(\n\u001b[0;32m      3\u001b[0m     model,\n\u001b[0;32m      4\u001b[0m     args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer\n\u001b[0;32m      9\u001b[0m )\n\u001b[1;32m---> 10\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\[REDACTED]\\lib\\site-packages\\transformers\\trainer.py:1555\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1553\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1554\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1555\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1556\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1558\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1560\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\[REDACTED]\\lib\\site-packages\\transformers\\trainer.py:1922\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1919\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m steps_skipped) \u001b[38;5;241m/\u001b[39m steps_in_epoch\n\u001b[0;32m   1920\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m-> 1922\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1924\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_substep_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[1;32m~\\[REDACTED]\\lib\\site-packages\\transformers\\trainer.py:2271\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[1;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2269\u001b[0m         metrics\u001b[38;5;241m.\u001b[39mupdate(dataset_metrics)\n\u001b[0;32m   2270\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2271\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2272\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[0;32m   2274\u001b[0m \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "File \u001b[1;32m~\\[REDACTED]\\lib\\site-packages\\transformers\\trainer_seq2seq.py:165\u001b[0m, in \u001b[0;36mSeq2SeqTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix, **gen_kwargs)\u001b[0m\n\u001b[0;32m    162\u001b[0m     gen_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_beams\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgeneration_num_beams\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gen_kwargs \u001b[38;5;241m=\u001b[39m gen_kwargs\n\u001b[1;32m--> 165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\[REDACTED]\\lib\\site-packages\\transformers\\trainer.py:3011\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   3008\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m   3010\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[1;32m-> 3011\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3012\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3013\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3014\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[0;32m   3015\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[0;32m   3016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   3017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3019\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3021\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[0;32m   3022\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[1;32m~\\[REDACTED]\\lib\\site-packages\\transformers\\trainer.py:3232\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   3229\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mgather_for_metrics((labels))\n\u001b[0;32m   3230\u001b[0m     labels_host \u001b[38;5;241m=\u001b[39m labels \u001b[38;5;28;01mif\u001b[39;00m labels_host \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m nested_concat(labels_host, labels, padding_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m-> 3232\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallback_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_prediction_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontrol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3234\u001b[0m \u001b[38;5;66;03m# Gather all tensors and put them back on the CPU if we have done enough accumulation steps.\u001b[39;00m\n\u001b[0;32m   3235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   3236\u001b[0m     args\u001b[38;5;241m.\u001b[39meval_accumulation_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3237\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m args\u001b[38;5;241m.\u001b[39meval_accumulation_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   3238\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39msync_gradients \u001b[38;5;129;01mor\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(accelerate_version) \u001b[38;5;241m>\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.20.3\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m   3239\u001b[0m ):\n",
      "File \u001b[1;32m~\\[REDACTED]\\lib\\site-packages\\transformers\\trainer_callback.py:403\u001b[0m, in \u001b[0;36mCallbackHandler.on_prediction_step\u001b[1;34m(self, args, state, control)\u001b[0m\n\u001b[0;32m    402\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_prediction_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: TrainingArguments, state: TrainerState, control: TrainerControl):\n\u001b[1;32m--> 403\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_event\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mon_prediction_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontrol\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\[REDACTED]\\lib\\site-packages\\transformers\\trainer_callback.py:407\u001b[0m, in \u001b[0;36mCallbackHandler.call_event\u001b[1;34m(self, event, args, state, control, **kwargs)\u001b[0m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_event\u001b[39m(\u001b[38;5;28mself\u001b[39m, event, args, state, control, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    406\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[1;32m--> 407\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(callback, event)(\n\u001b[0;32m    408\u001b[0m             args,\n\u001b[0;32m    409\u001b[0m             state,\n\u001b[0;32m    410\u001b[0m             control,\n\u001b[0;32m    411\u001b[0m             model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[0;32m    412\u001b[0m             tokenizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer,\n\u001b[0;32m    413\u001b[0m             optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer,\n\u001b[0;32m    414\u001b[0m             lr_scheduler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler,\n\u001b[0;32m    415\u001b[0m             train_dataloader\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataloader,\n\u001b[0;32m    416\u001b[0m             eval_dataloader\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_dataloader,\n\u001b[0;32m    417\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    418\u001b[0m         )\n\u001b[0;32m    419\u001b[0m         \u001b[38;5;66;03m# A Callback can skip the return of `control` if it doesn't change it.\u001b[39;00m\n\u001b[0;32m    420\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\[REDACTED]\\lib\\site-packages\\transformers\\utils\\notebook.py:318\u001b[0m, in \u001b[0;36mNotebookProgressCallback.on_prediction_step\u001b[1;34m(self, args, state, control, eval_dataloader, **kwargs)\u001b[0m\n\u001b[0;32m    316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_bar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 318\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprediction_bar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprediction_bar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\[REDACTED]\\lib\\site-packages\\transformers\\utils\\notebook.py:161\u001b[0m, in \u001b[0;36mNotebookProgressBar.update\u001b[1;34m(self, value, force_update, comment)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maverage_time_per_item \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredicted_remaining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maverage_time_per_item \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;241m-\u001b[39m value)\n\u001b[1;32m--> 161\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_bar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_value \u001b[38;5;241m=\u001b[39m value\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_time \u001b[38;5;241m=\u001b[39m current_time\n",
      "File \u001b[1;32m~\\[REDACTED]\\lib\\site-packages\\transformers\\utils\\notebook.py:180\u001b[0m, in \u001b[0;36mNotebookProgressBar.update_bar\u001b[1;34m(self, value, comment)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    177\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspaced_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mformat_time(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39melapsed_time)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m <\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    178\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mformat_time(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredicted_remaining)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    179\u001b[0m     )\n\u001b[1;32m--> 180\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maverage_time_per_item\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m it/s\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomment \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomment) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomment\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplay()\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "model = transformers.AutoModelForSeq2SeqLM.from_config(model_config)\n",
    "trainer = transformers.Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['test'],\n",
    "    data_collator=transformers.DataCollatorForSeq2Seq(tokenizer, model=model),\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff48bdb1",
   "metadata": {},
   "source": [
    "# The problem &uarr;\n",
    "\n",
    "# The fix &darr;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e95cc173",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.utils.notebook import format_time\n",
    "\n",
    "def _update_bar(self, value, comment=None):\n",
    "    spaced_value = \" \" * (len(str(self.total)) - len(str(value))) + str(value)\n",
    "    if self.elapsed_time is None:\n",
    "        self.label = f\"[{spaced_value}/{self.total} : < :\"\n",
    "    elif self.predicted_remaining is None:\n",
    "        self.label = f\"[{spaced_value}/{self.total} {format_time(self.elapsed_time)}\"\n",
    "    else:\n",
    "        self.label = (\n",
    "            f\"[{spaced_value}/{self.total} {format_time(self.elapsed_time)} <\"\n",
    "            f\" {format_time(self.predicted_remaining)}\"\n",
    "        )\n",
    "        if self.average_time_per_item == 0:\n",
    "            self.label += \", +inf it/s\"\n",
    "        else:\n",
    "            self.label += f\", {1/self.average_time_per_item:.2f} it/s\"\n",
    "    self.label += \"]\" if self.comment is None or len(self.comment) == 0 else f\", {self.comment}]\"\n",
    "    self.display()\n",
    "    \n",
    "transformers.utils.notebook.NotebookProgressBar.update_bar = _update_bar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29569c8d",
   "metadata": {},
   "source": [
    "old\n",
    "```\n",
    "self.label += f\", {1/self.average_time_per_item:.2f} it/s\"\n",
    "```\n",
    "new\n",
    "```\n",
    "if self.average_time_per_item == 0:\n",
    "    self.label += \", +inf it/s\"\n",
    "else:\n",
    "    self.label += f\", {1/self.average_time_per_item:.2f} it/s\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bbdb1f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _update(self, value: int, force_update: bool = False, comment: str = None):\n",
    "        \"\"\"\n",
    "        The main method to update the progress bar to `value`.\n",
    "\n",
    "        Args:\n",
    "            value (`int`):\n",
    "                The value to use. Must be between 0 and `total`.\n",
    "            force_update (`bool`, *optional*, defaults to `False`):\n",
    "                Whether or not to force and update of the internal state and display (by default, the bar will wait for\n",
    "                `value` to reach the value it predicted corresponds to a time of more than the `update_every` attribute\n",
    "                since the last update to avoid adding boilerplate).\n",
    "            comment (`str`, *optional*):\n",
    "                A comment to add on the left of the progress bar.\n",
    "        \"\"\"\n",
    "        self.value = value\n",
    "        if comment is not None:\n",
    "            self.comment = comment\n",
    "        if self.last_value is None:\n",
    "            self.start_time = self.last_time = time.time()\n",
    "            self.start_value = self.last_value = value\n",
    "            self.elapsed_time = self.predicted_remaining = None\n",
    "            self.first_calls = self.warmup\n",
    "            self.wait_for = 1\n",
    "            self.update_bar(value)\n",
    "        elif value <= self.last_value and not force_update:\n",
    "            return\n",
    "        elif force_update or self.first_calls > 0 or value >= min(self.last_value + self.wait_for, self.total):\n",
    "            if self.first_calls > 0:\n",
    "                self.first_calls -= 1\n",
    "            current_time = time.time()\n",
    "            self.elapsed_time = current_time - self.start_time\n",
    "            # We could have value = self.start_value if the update is called twixe with the same start value.\n",
    "            if value > self.start_value:\n",
    "                self.average_time_per_item = self.elapsed_time / (value - self.start_value)\n",
    "            else:\n",
    "                self.average_time_per_item = None\n",
    "            if value >= self.total:\n",
    "                value = self.total\n",
    "                self.predicted_remaining = None\n",
    "                if not self.leave:\n",
    "                    self.close()\n",
    "            elif self.average_time_per_item is not None:\n",
    "                self.predicted_remaining = self.average_time_per_item * (self.total - value)\n",
    "            self.update_bar(value)\n",
    "            self.last_value = value\n",
    "            self.last_time = current_time\n",
    "            if self.average_time_per_item is None or self.average_time_per_item == 0:\n",
    "                self.wait_for = 1\n",
    "            else:\n",
    "                self.wait_for = max(int(self.update_every / self.average_time_per_item), 1)\n",
    "\n",
    "transformers.utils.notebook.NotebookProgressBar.update = _update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40f5d6b",
   "metadata": {},
   "source": [
    "old\n",
    "```\n",
    "if self.average_time_per_item is None:\n",
    "```\n",
    "new\n",
    "```\n",
    "if self.average_time_per_item is None or self.average_time_per_item == 0:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28a8b620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='554' max='554' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [554/554 00:18, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.100100</td>\n",
       "      <td>0.048945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.053900</td>\n",
       "      <td>0.035581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.042100</td>\n",
       "      <td>0.024348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.030600</td>\n",
       "      <td>0.018407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.017823</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=554, training_loss=0.11943072145165949, metrics={'train_runtime': 18.678, 'train_samples_per_second': 385.053, 'train_steps_per_second': 29.661, 'total_flos': 593675777280.0, 'train_loss': 0.11943072145165949, 'epoch': 1.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = transformers.AutoModelForSeq2SeqLM.from_config(model_config)\n",
    "trainer = transformers.Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['test'],\n",
    "    data_collator=transformers.DataCollatorForSeq2Seq(tokenizer, model=model),\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
