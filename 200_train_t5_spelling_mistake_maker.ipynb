{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "504dc62d",
   "metadata": {},
   "source": [
    "# Rough notebook to see if we can train a small model to make spelling mistakes\n",
    "\n",
    "## TODO\n",
    "\n",
    "- why are we using a character level tokenizer\n",
    "- are the spelling mistakes the model makes any good ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef354cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import transformers, json\n",
    "from pathlib import Path\n",
    "from datasets import Dataset\n",
    "from typing import Dict, List, Optional, Sequence, Union\n",
    "import time, random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d30162",
   "metadata": {},
   "source": [
    "## `CharacterTokenizer`\n",
    "\n",
    "Create a character level tokenizer based on [character tokenizer](https://raw.githubusercontent.com/dariush-bahrami/character-tokenizer/master/charactertokenizer/core.py) for BERT (which was inspired by the [CANINE](https://arxiv.org/abs/2103.06874) [tokenizer](https://github.com/huggingface/transformers/blob/main/src/transformers/models/canine/tokenization_canine.py)) and the [t5 tokenizer](https://github.com/huggingface/transformers/blob/main/src/transformers/models/t5/tokenization_t5.py)\n",
    "\n",
    "TODO:\n",
    "- not sure we need model_max_length\n",
    "- test all of the save, get, from ... functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce0df2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterTokenizer(transformers.tokenization_utils.PreTrainedTokenizer):\n",
    "    def __init__(self, characters: Sequence[str], model_max_length: int, **kwargs):\n",
    "        \"\"\"Character tokenizer for Hugging Face transformers.\n",
    "\n",
    "        Args:\n",
    "            characters (Sequence[str]): List of desired characters. Any character which\n",
    "                is not included in this list will be replaced by a special token <unk>.\n",
    "\n",
    "            model_max_length (int): Model maximum sequence length.\n",
    "        \"\"\"\n",
    "        self.characters = characters\n",
    "        self.model_max_length = model_max_length\n",
    "        pad_token = \"<pad>\"\n",
    "        unk_token = \"<unk>\"\n",
    "        eos_token = \"</s>\"\n",
    "        for token in [pad_token, unk_token, eos_token]:\n",
    "            transformers.tokenization_utils.AddedToken(token)\n",
    "        \n",
    "        self._vocab_str_to_int = {\n",
    "            pad_token: 0,\n",
    "            eos_token: 1,\n",
    "            unk_token: 2,\n",
    "            **{ch: i + 3 for i, ch in enumerate(characters)},\n",
    "        }\n",
    "        self._vocab_int_to_str = {v: k for k, v in self._vocab_str_to_int.items()}\n",
    "        \n",
    "        super().__init__(\n",
    "            eos_token=eos_token,\n",
    "            pad_token=pad_token,\n",
    "            unk_token=unk_token,\n",
    "            add_prefix_space=False,\n",
    "            model_max_length=model_max_length,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self) -> int:\n",
    "        return len(self._vocab_str_to_int)\n",
    "\n",
    "    def get_vocab(self):\n",
    "        return self._vocab_str_to_int\n",
    "    \n",
    "    def _tokenize(self, text: str) -> List[str]:\n",
    "        return list(text)\n",
    "\n",
    "    def _convert_token_to_id(self, token: str) -> int:\n",
    "        return self._vocab_str_to_int.get(token, 2) # default to unk_token <unk>\n",
    "\n",
    "    def _convert_id_to_token(self, index: int) -> str:\n",
    "        return self._vocab_int_to_str[index]\n",
    "\n",
    "    def convert_tokens_to_string(self, tokens):\n",
    "        return \"\".join(tokens)\n",
    "\n",
    "    def _add_eos_if_not_present(self, token_ids: List[int]) -> List[int]:\n",
    "        \"\"\"Do not add eos again if user already added it.\"\"\"\n",
    "        if len(token_ids) > 0 and token_ids[-1] == self.eos_token_id:\n",
    "            return token_ids\n",
    "        else:\n",
    "            return token_ids + [self.eos_token_id]\n",
    "        \n",
    "    def build_inputs_with_special_tokens(\n",
    "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
    "    ) -> List[int]:\n",
    "        \"\"\"\n",
    "        Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n",
    "        adding special tokens. A sequence has the following format:\n",
    "\n",
    "        - single sequence: `X </s>`\n",
    "        - pair of sequences: `A </s> B </s>`\n",
    "\n",
    "        Args:\n",
    "            token_ids_0 (`List[int]`):\n",
    "                List of IDs to which the special tokens will be added.\n",
    "            token_ids_1 (`List[int]`, *optional*):\n",
    "                Optional second list of IDs for sequence pairs.\n",
    "\n",
    "        Returns:\n",
    "            `List[int]`: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.\n",
    "        \"\"\"\n",
    "        token_ids_0 = self._add_eos_if_not_present(token_ids_0)\n",
    "        if token_ids_1 is None:\n",
    "            return token_ids_0\n",
    "        else:\n",
    "            token_ids_1 = self._add_eos_if_not_present(token_ids_1)\n",
    "            return token_ids_0 + token_ids_1\n",
    "        \n",
    "    def get_special_tokens_mask(\n",
    "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None, already_has_special_tokens: bool = False\n",
    "    ) -> List[int]:\n",
    "        \"\"\"\n",
    "        Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding\n",
    "        special tokens using the tokenizer `prepare_for_model` method.\n",
    "\n",
    "        Args:\n",
    "            token_ids_0 (`List[int]`):\n",
    "                List of IDs.\n",
    "            token_ids_1 (`List[int]`, *optional*):\n",
    "                Optional second list of IDs for sequence pairs.\n",
    "            already_has_special_tokens (`bool`, *optional*, defaults to `False`):\n",
    "                Whether or not the token list is already formatted with special tokens for the model.\n",
    "\n",
    "        Returns:\n",
    "            `List[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n",
    "        \"\"\"\n",
    "        if already_has_special_tokens:\n",
    "            return super().get_special_tokens_mask(\n",
    "                token_ids_0=token_ids_0, token_ids_1=token_ids_1, already_has_special_tokens=True\n",
    "            )\n",
    "\n",
    "        # normal case: some special tokens\n",
    "        if token_ids_1 is None:\n",
    "            return ([0] * len(token_ids_0)) + [1]\n",
    "        return ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n",
    "\n",
    "    def create_token_type_ids_from_sequences(\n",
    "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
    "    ) -> List[int]:\n",
    "        \"\"\"\n",
    "        Create a mask from the two sequences passed to be used in a sequence-pair classification task. T5 does not make\n",
    "        use of token type ids, therefore a list of zeros is returned.\n",
    "\n",
    "        Args:\n",
    "            token_ids_0 (`List[int]`):\n",
    "                List of IDs.\n",
    "            token_ids_1 (`List[int]`, *optional*):\n",
    "                Optional second list of IDs for sequence pairs.\n",
    "\n",
    "        Returns:\n",
    "            `List[int]`: List of zeros.\n",
    "        \"\"\"\n",
    "        eos = [self.eos_token_id]\n",
    "\n",
    "        if token_ids_1 is None:\n",
    "            return len(token_ids_0 + eos) * [0]\n",
    "        return len(token_ids_0 + eos + token_ids_1 + eos) * [0]\n",
    "\n",
    "    def get_config(self) -> Dict:\n",
    "        return {\n",
    "            \"char_ords\": [ord(ch) for ch in self.characters],\n",
    "            \"model_max_length\": self.model_max_length,\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config: Dict) -> \"CharacterTokenizer\":\n",
    "        cfg = {}\n",
    "        cfg[\"characters\"] = [chr(i) for i in config[\"char_ords\"]]\n",
    "        cfg[\"model_max_length\"] = config[\"model_max_length\"]\n",
    "        return cls(**cfg)\n",
    "\n",
    "    def save_pretrained(self, save_directory: Union[str, os.PathLike], **kwargs):\n",
    "        cfg_file = Path(save_directory) / \"tokenizer_config.json\"\n",
    "        cfg = self.get_config()\n",
    "        with open(cfg_file, \"w\") as f:\n",
    "            json.dump(cfg, f, indent=4)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, save_directory: Union[str, os.PathLike], **kwargs):\n",
    "        cfg_file = Path(save_directory) / \"tokenizer_config.json\"\n",
    "        with open(cfg_file) as f:\n",
    "            cfg = json.load(f)\n",
    "        return cls.from_config(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77d7842b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized\n",
      "{'input_ids': [2, 4, 5, 6, 7, 8, 9, 2, 3, 2, 7, 2, 2, 2, 1], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "input_ids converted to tokens\n",
      "['<unk>', 'B', 'C', 'D', 'E', 'F', 'G', '<unk>', ' ', '<unk>', 'E', '<unk>', '<unk>', '<unk>', '</s>']\n"
     ]
    }
   ],
   "source": [
    "def demo():\n",
    "    tokenizer = CharacterTokenizer(' BCDEFG', 512)\n",
    "    tokenized = tokenizer('ABCDEFGH HELLO')\n",
    "    print('tokenized')\n",
    "    print(tokenized)\n",
    "    print('\\ninput_ids converted to tokens')\n",
    "    print(tokenizer.convert_ids_to_tokens(tokenized['input_ids']))\n",
    "demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16b9502",
   "metadata": {},
   "source": [
    "we're just using uppercase characters and hyphen for now ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b049c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '-']\n"
     ]
    }
   ],
   "source": [
    "characters = [chr(i) for i in range(65, 91)] + ['-']\n",
    "print(characters)\n",
    "tokenizer = CharacterTokenizer(characters, 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba6e8ad",
   "metadata": {},
   "source": [
    "```\n",
    "transformers.models.t5.configuration_t5.T5Config(\n",
    "    vocab_size=32128,\n",
    "    d_model=512,\n",
    "    d_kv=64,\n",
    "    d_ff=2048,\n",
    "    num_layers=6,\n",
    "    num_decoder_layers=None,\n",
    "    num_heads=8,\n",
    "    relative_attention_num_buckets=32,\n",
    "    relative_attention_max_distance=128,\n",
    "    dropout_rate=0.1,\n",
    "    layer_norm_epsilon=1e-06,\n",
    "    initializer_factor=1.0,\n",
    "    feed_forward_proj='relu',\n",
    "    is_encoder_decoder=True,\n",
    "    use_cache=True,\n",
    "    pad_token_id=0,\n",
    "    eos_token_id=1,\n",
    "    **kwargs,\n",
    ")\n",
    "Docstring:     \n",
    "This is the configuration class to store the configuration of a [`T5Model`] or a [`TFT5Model`]. It is used to\n",
    "instantiate a T5 model according to the specified arguments, defining the model architecture. Instantiating a\n",
    "configuration with the defaults will yield a similar configuration to that of the T5\n",
    "[t5-small](https://huggingface.co/t5-small) architecture.\n",
    "\n",
    "Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n",
    "documentation from [`PretrainedConfig`] for more information.\n",
    "\n",
    "Arguments:\n",
    "    vocab_size (`int`, *optional*, defaults to 32128):\n",
    "        Vocabulary size of the T5 model. Defines the number of different tokens that can be represented by the\n",
    "        `inputs_ids` passed when calling [`T5Model`] or [`TFT5Model`].\n",
    "    d_model (`int`, *optional*, defaults to 512):\n",
    "        Size of the encoder layers and the pooler layer.\n",
    "    d_kv (`int`, *optional*, defaults to 64):\n",
    "        Size of the key, query, value projections per attention head. The `inner_dim` of the projection layer will\n",
    "        be defined as `num_heads * d_kv`.\n",
    "    d_ff (`int`, *optional*, defaults to 2048):\n",
    "        Size of the intermediate feed forward layer in each `T5Block`.\n",
    "    num_layers (`int`, *optional*, defaults to 6):\n",
    "        Number of hidden layers in the Transformer encoder.\n",
    "    num_decoder_layers (`int`, *optional*):\n",
    "        Number of hidden layers in the Transformer decoder. Will use the same value as `num_layers` if not set.\n",
    "    num_heads (`int`, *optional*, defaults to 8):\n",
    "        Number of attention heads for each attention layer in the Transformer encoder.\n",
    "    relative_attention_num_buckets (`int`, *optional*, defaults to 32):\n",
    "        The number of buckets to use for each attention layer.\n",
    "    relative_attention_max_distance (`int`, *optional*, defaults to 128):\n",
    "        The maximum distance of the longer sequences for the bucket separation.\n",
    "    dropout_rate (`float`, *optional*, defaults to 0.1):\n",
    "        The ratio for all dropout layers.\n",
    "    layer_norm_eps (`float`, *optional*, defaults to 1e-6):\n",
    "        The epsilon used by the layer normalization layers.\n",
    "    initializer_factor (`float`, *optional*, defaults to 1):\n",
    "        A factor for initializing all weight matrices (should be kept to 1, used internally for initialization\n",
    "        testing).\n",
    "    feed_forward_proj (`string`, *optional*, defaults to `\"relu\"`):\n",
    "        Type of feed forward layer to be used. Should be one of `\"relu\"` or `\"gated-gelu\"`. T5v1.1 uses the\n",
    "        `\"gated-gelu\"` feed forward projection. Original T5 uses `\"relu\"`.\n",
    "    use_cache (`bool`, *optional*, defaults to `True`):\n",
    "        Whether or not the model should return the last key/values attentions (not used by all models).\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "755f99b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# go for a v-small model of ~1m parameters - T5-Small (60 million parameters)\n",
    "# TODO: not sure if these numbers give us the best model for the param budget ...\n",
    "model_config = transformers.models.t5.configuration_t5.T5Config(\n",
    "    decoder_start_token_id=tokenizer.pad_token_id,\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    d_model=128,                        # \"d_model\": 512,\n",
    "    d_kv=16,                            # \"d_kv\": 64,\n",
    "    d_ff=512,                           # \"d_ff\": 2048,\n",
    "    num_layers=3,                       # \"num_layers\": 6,\n",
    "    num_heads=4,                        # \"num_heads\": 8,\n",
    "    relative_attention_num_buckets=8,   # \"relative_attention_num_buckets\": 32,\n",
    "    relative_attention_max_distance=32, # relative_attention_max_distance (`int`, *optional*, defaults to 128):\n",
    "#     n_positions=128 #   \"n_positions\": 512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0614c3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    print('trainable model parameters:', trainable_model_params)\n",
    "    print('all model parameters:', all_model_params)\n",
    "    print(f'percentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "130e63a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 1087424\n",
      "all model parameters: 1087424\n",
      "percentage of trainable model parameters: 100.00%\n"
     ]
    }
   ],
   "source": [
    "print_number_of_trainable_model_parameters(transformers.AutoModelForSeq2SeqLM.from_config(model_config))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ddbd73",
   "metadata": {},
   "source": [
    "BioBERT has ~110m parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0467a836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>correct</th>\n",
       "      <th>mistake</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PLATFORM</td>\n",
       "      <td>PLAFFORM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PLATFORM</td>\n",
       "      <td>PLATFORN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EXISTING</td>\n",
       "      <td>EXISTSING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PYTHON</td>\n",
       "      <td>PHYTON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FOLLOWING</td>\n",
       "      <td>FOLOWING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>TRACHEA</td>\n",
       "      <td>TRACHEIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>VAGINA</td>\n",
       "      <td>VAGNIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>VERTEBRA</td>\n",
       "      <td>VERTEBRE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>VOMIT</td>\n",
       "      <td>VOMMIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>YEAST</td>\n",
       "      <td>YEEST</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>167 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       correct    mistake\n",
       "0     PLATFORM   PLAFFORM\n",
       "1     PLATFORM   PLATFORN\n",
       "2     EXISTING  EXISTSING\n",
       "3       PYTHON     PHYTON\n",
       "4    FOLLOWING   FOLOWING\n",
       "..         ...        ...\n",
       "125    TRACHEA   TRACHEIA\n",
       "126     VAGINA     VAGNIA\n",
       "127   VERTEBRA   VERTEBRE\n",
       "128      VOMIT     VOMMIT\n",
       "129      YEAST      YEEST\n",
       "\n",
       "[167 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eval = pd.concat([\n",
    "    pd.read_csv('data/my-spelling-mistakes.txt', sep=' '),\n",
    "    pd.read_csv('data/common_medical_misspellings.csv')\n",
    "])\n",
    "for c in df_eval.columns:\n",
    "    df_eval[c] = df_eval[c].str.upper()\n",
    "df_eval = df_eval.drop_duplicates()\n",
    "df_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54e6b9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39101\n",
      "38437\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>correct</th>\n",
       "      <th>mistake</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7781</th>\n",
       "      <td>CONSCIOUS</td>\n",
       "      <td>CONNIES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7415</th>\n",
       "      <td>CONDITIONS</td>\n",
       "      <td>GONES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13680</th>\n",
       "      <td>EXPLORATORY</td>\n",
       "      <td>EXPLARATORY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19438</th>\n",
       "      <td>LEEDS</td>\n",
       "      <td>LESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3643</th>\n",
       "      <td>AUDITORIUM</td>\n",
       "      <td>AUDITORIM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           correct      mistake\n",
       "7781     CONSCIOUS      CONNIES\n",
       "7415    CONDITIONS        GONES\n",
       "13680  EXPLORATORY  EXPLARATORY\n",
       "19438        LEEDS         LESS\n",
       "3643    AUDITORIUM    AUDITORIM"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.concat([pd.read_csv(f'data/{f}.csv') for f in [\n",
    "    'RogerMitton/roger_mitton_common_misspellings', 'wikipedia/wikipedia_common_misspellings']])\n",
    "df_train = df_train.drop_duplicates()\n",
    "print(len(df_train))\n",
    "df_train = df_train[~df_train['correct'].isin(df_eval['correct'])]\n",
    "print(len(df_train))\n",
    "df_train.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adfdc329",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = None # 128??\n",
    "max_target_length = None # 128?\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    model_inputs = tokenizer(examples['correct'], max_length=max_input_length, truncation=True)\n",
    "    labels = tokenizer(examples['mistake'], max_length=max_target_length, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1f386d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf1c6b8f335a442286a0dd336e7299e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/38437 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d4d7ebd150946f68397bc52cf7a0b8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/167 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def mk_dataset(df):\n",
    "    return Dataset.from_pandas(df, preserve_index=False).map(\n",
    "        preprocess_function, batched=True, remove_columns=['correct', 'mistake'])\n",
    "train_dataset, eval_dataset = [mk_dataset(df) for df in [df_train, df_eval]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7da40672",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 167\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "449802f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_model_name = f'models/t5-quick'\n",
    "args = transformers.Seq2SeqTrainingArguments(\n",
    "    finetune_model_name,\n",
    "    optim=\"adamw_torch\",\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=100,\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-3,\n",
    "    per_device_train_batch_size=256, # large batch size is fast but doesn't learn much\n",
    "    per_device_eval_batch_size=64,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=15, # TODO: xxx\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    push_to_hub=False,\n",
    "    logging_steps=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43fb689b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.utils.notebook import format_time\n",
    "\n",
    "def _update(self, value: int, force_update: bool = False, comment: str = None):\n",
    "        \"\"\"\n",
    "        The main method to update the progress bar to `value`.\n",
    "\n",
    "        Args:\n",
    "            value (`int`):\n",
    "                The value to use. Must be between 0 and `total`.\n",
    "            force_update (`bool`, *optional*, defaults to `False`):\n",
    "                Whether or not to force and update of the internal state and display (by default, the bar will wait for\n",
    "                `value` to reach the value it predicted corresponds to a time of more than the `update_every` attribute\n",
    "                since the last update to avoid adding boilerplate).\n",
    "            comment (`str`, *optional*):\n",
    "                A comment to add on the left of the progress bar.\n",
    "        \"\"\"\n",
    "        self.value = value\n",
    "        if comment is not None:\n",
    "            self.comment = comment\n",
    "        if self.last_value is None:\n",
    "            self.start_time = self.last_time = time.time()\n",
    "            self.start_value = self.last_value = value\n",
    "            self.elapsed_time = self.predicted_remaining = None\n",
    "            self.first_calls = self.warmup\n",
    "            self.wait_for = 1\n",
    "            self.update_bar(value)\n",
    "        elif value <= self.last_value and not force_update:\n",
    "            return\n",
    "        elif force_update or self.first_calls > 0 or value >= min(self.last_value + self.wait_for, self.total):\n",
    "            if self.first_calls > 0:\n",
    "                self.first_calls -= 1\n",
    "            current_time = time.time()\n",
    "            self.elapsed_time = current_time - self.start_time\n",
    "            # We could have value = self.start_value if the update is called twixe with the same start value.\n",
    "            if value > self.start_value:\n",
    "                self.average_time_per_item = self.elapsed_time / (value - self.start_value)\n",
    "            else:\n",
    "                self.average_time_per_item = None\n",
    "            if value >= self.total:\n",
    "                value = self.total\n",
    "                self.predicted_remaining = None\n",
    "                if not self.leave:\n",
    "                    self.close()\n",
    "            elif self.average_time_per_item is not None:\n",
    "                self.predicted_remaining = self.average_time_per_item * (self.total - value)\n",
    "            self.update_bar(value)\n",
    "            self.last_value = value\n",
    "            self.last_time = current_time\n",
    "            if self.average_time_per_item is None or self.average_time_per_item == 0:\n",
    "                self.wait_for = 1\n",
    "            else:\n",
    "                self.wait_for = max(int(self.update_every / self.average_time_per_item), 1)\n",
    "\n",
    "def _update_bar(self, value, comment=None):\n",
    "    spaced_value = \" \" * (len(str(self.total)) - len(str(value))) + str(value)\n",
    "    if self.elapsed_time is None:\n",
    "        self.label = f\"[{spaced_value}/{self.total} : < :\"\n",
    "    elif self.predicted_remaining is None:\n",
    "        self.label = f\"[{spaced_value}/{self.total} {format_time(self.elapsed_time)}\"\n",
    "    else:\n",
    "        self.label = (\n",
    "            f\"[{spaced_value}/{self.total} {format_time(self.elapsed_time)} <\"\n",
    "            f\" {format_time(self.predicted_remaining)}\"\n",
    "        )\n",
    "        if self.average_time_per_item == 0:\n",
    "            self.label += \", +inf it/s\"\n",
    "        else:\n",
    "            self.label += f\", {1/self.average_time_per_item:.2f} it/s\"\n",
    "    self.label += \"]\" if self.comment is None or len(self.comment) == 0 else f\", {self.comment}]\"\n",
    "    self.display()\n",
    "    \n",
    "transformers.utils.notebook.NotebookProgressBar.update = _update\n",
    "transformers.utils.notebook.NotebookProgressBar.update_bar = _update_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97233d9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2265' max='2265' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2265/2265 01:29, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.995900</td>\n",
       "      <td>1.992876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.712200</td>\n",
       "      <td>1.835418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.575600</td>\n",
       "      <td>1.725214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.418200</td>\n",
       "      <td>1.497420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.325200</td>\n",
       "      <td>1.347561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.237300</td>\n",
       "      <td>1.138963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.168000</td>\n",
       "      <td>0.999473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.117900</td>\n",
       "      <td>0.853691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.070000</td>\n",
       "      <td>0.809013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.042800</td>\n",
       "      <td>0.766690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.015400</td>\n",
       "      <td>0.742068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.013200</td>\n",
       "      <td>0.673822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.989400</td>\n",
       "      <td>0.731464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.966900</td>\n",
       "      <td>0.661758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.971100</td>\n",
       "      <td>0.662849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.945000</td>\n",
       "      <td>0.642421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.942200</td>\n",
       "      <td>0.624514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.944400</td>\n",
       "      <td>0.619348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.927200</td>\n",
       "      <td>0.610307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.918200</td>\n",
       "      <td>0.613160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.918200</td>\n",
       "      <td>0.612691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.907500</td>\n",
       "      <td>0.613181</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2265, training_loss=1.159190054129291, metrics={'train_runtime': 91.0563, 'train_samples_per_second': 6331.849, 'train_steps_per_second': 24.875, 'total_flos': 60098017566720.0, 'train_loss': 1.159190054129291, 'epoch': 15.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = transformers.AutoModelForSeq2SeqLM.from_config(model_config)\n",
    "trainer = transformers.Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=transformers.DataCollatorForSeq2Seq(tokenizer, model=model), # dynamic padding for efficiency\n",
    "    tokenizer=tokenizer,\n",
    "#     compute_metrics=compute_metrics, # TODO: see if metric other than loss helps\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a6801ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = transformers.pipeline('text2text-generation', model.cpu(), tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15380ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONCENTRATE\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'CONSENTERATE'},\n",
       " {'generated_text': 'CONSENTRATE'},\n",
       " {'generated_text': 'CONCENTRATE'},\n",
       " {'generated_text': 'CONCENTRATE'},\n",
       " {'generated_text': 'CONCENTRATE'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one_from_the_train_set = 'YORKSHIRE'\n",
    "one_from_the_train_set = random.choice(df_train['correct'].to_list())\n",
    "print(one_from_the_train_set)\n",
    "pipe([one_from_the_train_set]*5, max_new_tokens=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f91e7fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NETWORK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'NETWORK'},\n",
       " {'generated_text': 'NETORK'},\n",
       " {'generated_text': 'NETWERK'},\n",
       " {'generated_text': 'NETWORK'},\n",
       " {'generated_text': 'NETWORK'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one_from_the_eval_set = 'ASPIRIN'\n",
    "one_from_the_eval_set = random.choice(df_eval['correct'].to_list())\n",
    "print(one_from_the_eval_set)\n",
    "generation_config = transformers.generation.GenerationConfig(do_sample=False)\n",
    "# generation_config = transformers.generation.GenerationConfig(do_sample=True, temperature=0.01, num_beams=1)\n",
    "pipe([one_from_the_eval_set]*5, generation_config=generation_config, max_new_tokens=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbf11ec",
   "metadata": {},
   "source": [
    "TODO: work out how to do greedy generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54f0173e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad>MORTGAGE</s>'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer('MORTGAGE', return_tensors='pt')['input_ids']\n",
    "# model.generate(input_ids, max_new_tokens=128)\n",
    "tokenizer.decode(model.generate(input_ids, max_new_tokens=128)[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
